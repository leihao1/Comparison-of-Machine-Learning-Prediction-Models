{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "- dimensions reduce\n",
    "- feature selection\n",
    "- maxmin scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- all randomsearch same ? all preprocessing same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import scikitplot as skplt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct,WhiteKernel,RBF,Matern,RationalQuadratic,ExpSineSquared,ConstantKernel,PairwiseKernel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(X_train, y_train, X_test, y_test, all_regrs, regr_names):\n",
    "    plt.figure(figsize=(18,8))\n",
    "    ax1 = plt.subplot(121)\n",
    "    mse_scores = plot_mse_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax1)\n",
    "    ax2 = plt.subplot(122)\n",
    "    r2_scores = plot_r2_score(X_train, y_train, X_test, y_test, all_regrs, regr_names,ax2)\n",
    "    plt.savefig(IMAGE_PATH+file.split('.')[0]+'_mse-r2')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mse_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax):\n",
    "    mse_scores = dict()\n",
    "    training_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for regr, regr_name in zip(all_regrs, regr_names):\n",
    "        train_preds = regr.predict(X_train)\n",
    "        test_preds = regr.predict(X_test)\n",
    "#         if y_test.dtype == 'int64' or y_test.dtype == 'int32':\n",
    "#             train_preds = [round(p) for p in train_preds]\n",
    "#             test_preds = [round(p) for p in test_preds]\n",
    "        train_score = sklearn.metrics.mean_squared_error(y_train, train_preds)\n",
    "        test_score = sklearn.metrics.mean_squared_error(y_test, test_preds)\n",
    "        training_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        mse_scores[regr_name] = test_score\n",
    "        \n",
    "    N = len(all_regrs)\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    p1 = plt.barh(ind-width/2, training_scores, align='center', label='Training Set', height=width)\n",
    "    p2 = plt.barh(ind+width/2, test_scores, align='center', label='Test Set', height=width)\n",
    "    for i, v in enumerate(training_scores):\n",
    "        plt.text(v+0.07,ind[i]-width/2.5,'%.3f'%v)\n",
    "        plt.text(test_scores[i]+0.07,ind[i]+width/1.5,'%.3f'%test_scores[i])\n",
    "        \n",
    "    plt.yticks(ind, regr_names) \n",
    "    plt.xlabel('MSE')\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.title('Mean Squared Error Of All Regressors')\n",
    "    plt.legend(handles=[p1,p2])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().invert_xaxis()\n",
    "#     ax.yaxis.tick_right()\n",
    "    return mse_scores\n",
    "\n",
    "def plot_r2_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax):\n",
    "    r2_scores = dict()\n",
    "    training_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for regr, regr_name in zip(all_regrs, regr_names):\n",
    "        train_preds = regr.predict(X_train)\n",
    "        test_preds = regr.predict(X_test)\n",
    "#         if y_test.dtype == 'int64' or y_test.dtype == 'int32':\n",
    "#             train_preds = [round(p) for p in train_preds]\n",
    "#             test_preds = [round(p) for p in test_preds]\n",
    "        train_score = sklearn.metrics.r2_score(y_train, train_preds)\n",
    "        test_score = sklearn.metrics.r2_score(y_test, test_preds)\n",
    "        training_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        r2_scores[regr_name] = test_score\n",
    "        \n",
    "    N = len(all_regrs)\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "#     p1 = plt.bar(ind, training_scores, width)\n",
    "#     p2 = plt.bar(ind+width, test_scores, width)\n",
    "#     plt.ylabel('Scores')\n",
    "#     plt.title('Scores by group and gender')\n",
    "#     plt.xticks(ind, regr_names,rotation='vertical')\n",
    "#     plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#     plt.legend((p1[0], p2[0]), ('Training', 'Test'))\n",
    "\n",
    "    p1 = plt.barh(ind-width/2, training_scores, align='center', label='Training Set', height=width)\n",
    "    p2 = plt.barh(ind+width/2, test_scores, align='center', label='Test Set', height=width)\n",
    "    for i, v in enumerate(training_scores):\n",
    "        plt.text(v+0.01,ind[i]-width/2.5,'%.3f'%v)\n",
    "        plt.text(max(test_scores[i],0)+0.01,ind[i]+width/1.5,'%.3f'%test_scores[i])\n",
    "\n",
    "    plt.yticks(ind, regr_names)\n",
    "    plt.xlabel('R² Score')\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.title('R² Scores Of All regressors')\n",
    "    plt.legend(handles=[p1,p2])\n",
    "    plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_xaxis()\n",
    "    ax.yaxis.tick_right()\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processiong tools\n",
    "def normalize_data(X_train, X_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)    \n",
    "    return X_train, X_test\n",
    "\n",
    "def encode_labels(x_train, x_test, cols=None):\n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    df = pd.concat([x_train,x_test],axis=0)\n",
    "\n",
    "    if cols == -1:#only encode last col\n",
    "        if df.dtype == 'object':\n",
    "            s_df = df.iloc[:]\n",
    "            not_null_df = s_df.loc[s_df.notnull()]\n",
    "            label_encoder.fit(not_null_df)\n",
    "            try:\n",
    "                x_train.loc[:] = x_train.loc[:].astype('float')\n",
    "            except:\n",
    "                pass\n",
    "            if x_train.loc[:].dtype == 'object':\n",
    "                x_train.loc[:] = label_encoder.transform(x_train.loc[:])\n",
    "\n",
    "            try:\n",
    "                x_test.loc[:] = x_test.loc[:].astype('float')\n",
    "            except:\n",
    "                pass\n",
    "            if x_test.loc[:].dtype == 'object':\n",
    "                x_test.loc[:] = label_encoder.transform(x_test.loc[:])\n",
    "    else:\n",
    "        for i,t in enumerate(df.dtypes):\n",
    "            if t == 'object':\n",
    "                s_df = df.iloc[:,i]\n",
    "                not_null_df = s_df.loc[s_df.notnull()]\n",
    "                label_encoder.fit(not_null_df)                \n",
    "                try:\n",
    "                    x_train.iloc[:,i] = x_train.iloc[:,i].astype('float')\n",
    "                except:\n",
    "                    pass\n",
    "#                     print(\"can not convert %s th column in x_train to float\"%i) \n",
    "                if x_train.iloc[:,i].dtype == 'object':\n",
    "                    x_train.iloc[:,i] = x_train.iloc[:,i].apply(lambda x: label_encoder.transform([x])[0] if type(x) == str else x)                \n",
    "                \n",
    "                try:\n",
    "                    x_test.iloc[:,i] = x_test.iloc[:,i].astype('float')\n",
    "                except:\n",
    "                    pass\n",
    "#                     print(\"can not convert %s th column in x_test to float\"%i)\n",
    "                if x_test.iloc[:,i].dtype == 'object':\n",
    "                    x_test.iloc[:,i] = x_test.iloc[:,i].apply(lambda x: label_encoder.transform([x])[0] if type(x) == str else x)\n",
    "    return x_train, x_test\n",
    "\n",
    "#put class colunmn at end of dataframe\n",
    "def put_class_col_end(dataFrame):\n",
    "    cols = dataFrame.columns.tolist()\n",
    "    cols = cols[1:] + cols[:1]\n",
    "    return dataFrame[cols]\n",
    "\n",
    "#impute ? with given strategy\n",
    "def impute_value(x_train, x_test, strategy):\n",
    "    if strategy == None:\n",
    "        return x_train.dropna(), x_test.dropna()\n",
    "    else:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "        train_type_dic = dict()#save original train data type before impute\n",
    "        for i,t in enumerate(x_train.dtypes):\n",
    "            if t != 'object':\n",
    "                train_type_dic[i] = t\n",
    "        test_type_dic = dict()#save original test data type before impute\n",
    "        for i,t in enumerate(x_test.dtypes):\n",
    "            if t != 'object':\n",
    "                test_type_dic[i] = t        \n",
    "        x_train = pd.DataFrame(imp.fit_transform(x_train))\n",
    "        x_test = pd.DataFrame(imp.transform(x_test))\n",
    "        for key in train_type_dic:\n",
    "            x_train.iloc[:,key] = x_train.iloc[:,key].astype(train_type_dic[key])\n",
    "        for key in test_type_dic:\n",
    "            x_test.iloc[:,key] = x_test.iloc[:,key].astype(test_type_dic[key])\n",
    "        return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# scipy.stats.randint(1,20)\n",
    "# scipy.stats.reciprocal(1.0, 100.),\n",
    "# scipy.stats.uniform(0.75, 1.25),\n",
    "def train_SVR(X_train, y_train):\n",
    "    print('Training SVR ...')\n",
    "    svr = SVR()\n",
    "    param_distributions = {\n",
    "        'kernel' : [DotProduct(),WhiteKernel(),RBF(),Matern(),RationalQuadratic()],\n",
    "        'C' : scipy.stats.reciprocal(1.0, 10.),\n",
    "#         'epsilon' : scipy.stats.uniform(0.1, 0.5),\n",
    "#         'gamma' : scipy.stats.reciprocal(0.01, 0.1),\n",
    "    }\n",
    "    randcv = RandomizedSearchCV(svr,param_distributions,n_iter=20,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_DecisionTree(X_train, y_train):\n",
    "    print('Training DecisionTree ...')\n",
    "    tree = DecisionTreeRegressor(random_state=0)\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(10,500)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(tree,param_distributions,n_iter=30,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_RandomForest(X_train, y_train):\n",
    "    print('Training RandomForest ...')\n",
    "    forest = RandomForestRegressor(random_state=0, warm_start=True)\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(1,50),\n",
    "        'n_estimators' : scipy.stats.randint(100,200)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(forest,param_distributions,n_iter=10,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_AdaBoost(X_train, y_train):\n",
    "    print('Training AdaBoost ...')\n",
    "    boost = AdaBoostRegressor(random_state=0)\n",
    "    param_distributions = {\n",
    "        'loss' : ['linear', 'square', 'exponential'],\n",
    "        'learning_rate' : scipy.stats.uniform(0.75, 1.25),\n",
    "        'n_estimators' : scipy.stats.randint(40,100)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(boost,param_distributions,n_iter=30,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_GaussianProcess(X_train, y_train):\n",
    "    print('Training GaussianProcess ...')\n",
    "    gaussian = GaussianProcessRegressor(normalize_y=True, random_state=0)\n",
    "    param_distributions = {\n",
    "        'kernel' : [DotProduct(),WhiteKernel(),RBF(),Matern(),RationalQuadratic()],\n",
    "#         'n_restarts_optimizer' : scipy.stats.randint(0,10),\n",
    "#         'alpha' : scipy.stats.uniform(1e-9, 1e-8)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(gaussian,param_distributions,n_iter=5,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_LinearRegression(X_train,y_train):\n",
    "    print('Training LinearRegression ...')\n",
    "    linear = LinearRegression(n_jobs=-1)\n",
    "    param_distributions = {\n",
    "        'normalize' : [True,False]\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(linear,param_distributions,n_iter=2,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_NeuralNetwork(X_train, y_train):\n",
    "    print('Training NeuralNetwork ...')\n",
    "    nn = MLPRegressor(solver='adam', random_state=0, warm_start=True, activation='tanh')\n",
    "    param_distributions = {\n",
    "        'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "        'hidden_layer_sizes' : [(100,50,25),(200,100,50),(200,100,50,25)],\n",
    "        'learning_rate_init' : scipy.stats.uniform(0.001, 0.005),\n",
    "        'max_iter' : scipy.stats.randint(200,500)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(nn,param_distributions,n_iter=15,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_regrs(X_train, y_train, X_test, y_test):\n",
    "    all_regrs = []\n",
    "    regr_names = []\n",
    "\n",
    "    regr1 = train_SVR(X_train, y_train)\n",
    "    all_regrs.append(regr1.best_estimator_)\n",
    "    regr_names.append('SVR')\n",
    "\n",
    "    regr2 = train_DecisionTree(X_train, y_train)\n",
    "    all_regrs.append(regr2.best_estimator_)\n",
    "    regr_names.append('Decision Tree')\n",
    "\n",
    "    regr3 = train_RandomForest(X_train, y_train)\n",
    "    all_regrs.append(regr3.best_estimator_)\n",
    "    regr_names.append('Random Forest')\n",
    "\n",
    "    regr4 = train_AdaBoost(X_train, y_train)\n",
    "    all_regrs.append(regr4.best_estimator_)\n",
    "    regr_names.append('AdaBoost')\n",
    "\n",
    "    regr5 = train_GaussianProcess(X_train, y_train)\n",
    "    all_regrs.append(regr5.best_estimator_)\n",
    "    regr_names.append('Gaussian Process')\n",
    "\n",
    "    regr6 = train_LinearRegression(X_train, y_train)\n",
    "    all_regrs.append(regr6.best_estimator_)\n",
    "    regr_names.append('Linear Regression')\n",
    "\n",
    "    regr7 = train_NeuralNetwork(X_train, y_train)\n",
    "    all_regrs.append(regr7.best_estimator_)\n",
    "    regr_names.append('NeuralNetwork')\n",
    "\n",
    "    plot_all(X_train, y_train, X_test, y_test, all_regrs, regr_names)\n",
    "    return all_regrs, regr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sets\n",
    "DATA_PATH = 'dataset/'\n",
    "IMAGE_PATH = 'image/'\n",
    "files = []\n",
    "dfs = []\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "file1 = 'winequality-red.csv'\n",
    "df1 = pd.read_csv(DATA_PATH+file1, delimiter=';',header=0)\n",
    "dfs.append(df1)\n",
    "files.append(file1)\n",
    "\n",
    "file1_2 = 'winequality-white.csv'\n",
    "df1_2 = pd.read_csv(DATA_PATH+file1_2, delimiter=';',header=0)\n",
    "dfs.append(df1_2)\n",
    "files.append(file1_2)\n",
    "\n",
    "file2 = 'communities.data'\n",
    "df2 = pd.read_csv(DATA_PATH+file2, delimiter=',', header=None)\n",
    "dfs.append(df2)\n",
    "files.append(file2)\n",
    "\n",
    "file3 = 'qsar_aquatic_toxicity.csv'\n",
    "df3 = pd.read_csv(DATA_PATH+file3, delimiter=';', header=None)\n",
    "dfs.append(df3)\n",
    "files.append(file3)\n",
    "\n",
    "#overfitting?\n",
    "file4 = 'Parkinson Speech_train_data.txt'\n",
    "df4 = pd.read_csv(DATA_PATH+file4, delimiter=',', header=None)\n",
    "dfs.append(df4)\n",
    "files.append(file4)\n",
    "\n",
    "file5 = 'Facebook_dataset.csv'\n",
    "df5 = pd.read_csv(DATA_PATH+file5, delimiter=';', header=0)\n",
    "dfs.append(df5)\n",
    "files.append(file5)\n",
    "\n",
    "file6 = 'Bike Sharing_hour.csv'\n",
    "df6 = pd.read_csv(DATA_PATH+file6, delimiter=',', header=0, index_col=0)\n",
    "df6 = df6.drop(['dteday','casual','registered','yr','mnth'],axis=1)\n",
    "df6 = df6.sample(500,random_state=0)\n",
    "dfs.append(df6)\n",
    "files.append(file6)\n",
    "\n",
    "# g1 g2 g3 corelated\n",
    "file7 = 'student-por.csv'\n",
    "df7 = pd.read_csv(DATA_PATH+file7, delimiter=';', header=0)\n",
    "dfs.append(df7)\n",
    "files.append(file7)\n",
    "\n",
    "file8 = 'Concrete_Data.xls'\n",
    "df8 = pd.read_excel(DATA_PATH+file8, header=0)\n",
    "dfs.append(df8)\n",
    "files.append(file8)\n",
    "\n",
    "#241600 rows, last four y \n",
    "file9 = 'sgemm_product.csv'\n",
    "df9 = pd.read_csv(DATA_PATH+file9, delimiter=',', header=0)\n",
    "# df9 = df9.sample(5000,random_state=0)\n",
    "dfs.append(df9)\n",
    "files.append(file9)\n",
    "\n",
    "file10 = 'ACT2_competition_training.csv'\n",
    "df10 = pd.read_csv(DATA_PATH+file10, delimiter=',', header=0, index_col=0)\n",
    "df10['y'] = df10.Act\n",
    "df10 = df10.drop(['Act'],axis=1)\n",
    "dfs.append(df10)\n",
    "files.append(file10)\n",
    "\n",
    "file10_2 = 'ACT4_competition_training.csv'\n",
    "df10_2 = pd.read_csv(DATA_PATH+file10_2, delimiter=',', header=0, index_col=0)\n",
    "df10_2['y'] = df10_2.Act\n",
    "df10_2 = df10_2.drop(['Act'],axis=1)\n",
    "dfs.append(df10_2)\n",
    "files.append(file10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df5\n",
    "file = file5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 19 columns):\n",
      "Page total likes                                                       500 non-null int64\n",
      "Type                                                                   500 non-null object\n",
      "Category                                                               500 non-null int64\n",
      "Post Month                                                             500 non-null int64\n",
      "Post Weekday                                                           500 non-null int64\n",
      "Post Hour                                                              500 non-null int64\n",
      "Paid                                                                   499 non-null float64\n",
      "Lifetime Post Total Reach                                              500 non-null int64\n",
      "Lifetime Post Total Impressions                                        500 non-null int64\n",
      "Lifetime Engaged Users                                                 500 non-null int64\n",
      "Lifetime Post Consumers                                                500 non-null int64\n",
      "Lifetime Post Consumptions                                             500 non-null int64\n",
      "Lifetime Post Impressions by people who have liked your Page           500 non-null int64\n",
      "Lifetime Post reach by people who like your Page                       500 non-null int64\n",
      "Lifetime People who have liked your Page and engaged with your post    500 non-null int64\n",
      "comment                                                                500 non-null int64\n",
      "like                                                                   499 non-null float64\n",
      "share                                                                  496 non-null float64\n",
      "Total Interactions                                                     500 non-null int64\n",
      "dtypes: float64(3), int64(15), object(1)\n",
      "memory usage: 74.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    499\n",
       "True       1\n",
       "Name: Paid, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Paid.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVR ...\n",
      "Training DecisionTree ...\n",
      "Training RandomForest ...\n",
      "Training AdaBoost ...\n",
      "Training GaussianProcess ...\n",
      "Training LinearRegression ...\n",
      "Training NeuralNetwork ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 554, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 597, in _score\n    return _multimetric_score(estimator, X_test, y_test, scorer)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 627, in _multimetric_score\n    score = scorer(estimator, X_test, y_test)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/metrics/scorer.py\", line 240, in _passthrough_scorer\n    return estimator.score(*args, **kwargs)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/base.py\", line 410, in score\n    y_type, _, _, _ = _check_reg_targets(y, y_pred, None)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/metrics/regression.py\", line 79, in _check_reg_targets\n    y_pred = check_array(y_pred, ensure_2d=False)\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 542, in check_array\n    allow_nan=force_all_finite == 'allow-nan')\n  File \"/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n    raise ValueError(msg_err.format(type_err, X.dtype))\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-53e0045fc349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# y_train, y_test = encode_labels(y_train, y_test, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mregrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all_regrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-fe7199ead3ba>\u001b[0m in \u001b[0;36mrun_all_regrs\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mregr_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear Regression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mregr7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_NeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mall_regrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregr7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mregr_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NeuralNetwork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e22820e64b58>\u001b[0m in \u001b[0;36mtrain_NeuralNetwork\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     88\u001b[0m     randcv = sklearn.model_selection.RandomizedSearchCV(nn,param_distributions,n_iter=15,cv=3,n_jobs=-1,\n\u001b[1;32m     89\u001b[0m                                                         iid=False,random_state=0)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mrandcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = df.replace({'?' : np.nan})\n",
    "except:\n",
    "    print('no question mark found')\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "X_train, X_test = encode_labels(X_train,X_test)\n",
    "X_train, X_test = impute_value(X_train, X_test,'mean')\n",
    "X_train, X_test = normalize_data(X_train, X_test)\n",
    "# y_train, y_test = encode_labels(y_train, y_test, -1)\n",
    "regrs, names = run_all_regrs(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of SVR(C=1.365268824654346, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "    gamma='auto_deprecated', kernel=RationalQuadratic(alpha=1, length_scale=1),\n",
      "    max_iter=-1, shrinking=True, tol=0.001, verbose=False)>\n",
      "<bound method BaseEstimator.get_params of DecisionTreeRegressor(criterion='mse', max_depth=182, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=0, splitter='best')>\n",
      "<bound method BaseEstimator.get_params of RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=112,\n",
      "                      n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                      warm_start=True)>\n",
      "<bound method BaseEstimator.get_params of AdaBoostRegressor(base_estimator=None, learning_rate=1.5573676413333202,\n",
      "                  loss='linear', n_estimators=63, random_state=0)>\n",
      "<bound method BaseEstimator.get_params of GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "                         kernel=DotProduct(sigma_0=1), n_restarts_optimizer=0,\n",
      "                         normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "                         random_state=0)>\n",
      "<bound method BaseEstimator.get_params of LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)>\n",
      "<bound method BaseEstimator.get_params of MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100, 50, 25), learning_rate='constant',\n",
      "             learning_rate_init=0.0010939490021817757, max_iter=253,\n",
      "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "             power_t=0.5, random_state=0, shuffle=True, solver='sgd',\n",
      "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "             warm_start=True)>\n"
     ]
    }
   ],
   "source": [
    "for r in regrs:\n",
    "    print(r.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_all(X_train, y_train, X_test, y_test, regrs, names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
