{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Utils')\n",
    "from pre_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import scikitplot as skplt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct,WhiteKernel,RBF,Matern,RationalQuadratic,ExpSineSquared,ConstantKernel,PairwiseKernel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(X_train, y_train, X_test, y_test, all_regrs, regr_names, file_name):\n",
    "    plt.figure(figsize=(18,8))\n",
    "    plt.suptitle(\"Dataset: %s\"%file_name, size=16)\n",
    "    ax1 = plt.subplot(121)\n",
    "    mse_scores = plot_mse_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax1)\n",
    "    ax2 = plt.subplot(122)\n",
    "    ax2.set_xlim(0,1)\n",
    "    r2_scores = plot_r2_score(X_train, y_train, X_test, y_test, all_regrs, regr_names,ax2)\n",
    "    plt.savefig(IMAGE_PATH+file.split('.')[0]+'_mse-r2')\n",
    "    plt.show()\n",
    "    return mse_scores,r2_scores\n",
    "\n",
    "def plot_mse_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax):\n",
    "    mse_scores = dict()\n",
    "    training_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for regr, regr_name in zip(all_regrs, regr_names):\n",
    "        train_preds = regr.predict(X_train)\n",
    "        test_preds = regr.predict(X_test)\n",
    "        train_score = sklearn.metrics.mean_squared_error(y_train, train_preds)\n",
    "        test_score = sklearn.metrics.mean_squared_error(y_test, test_preds)\n",
    "        training_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        mse_scores[regr_name] = test_score\n",
    "        \n",
    "    N = len(all_regrs)\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    p1 = plt.barh(ind-width/2, training_scores, align='center', label='Training Set', height=width)\n",
    "    p2 = plt.barh(ind+width/2, test_scores, align='center', label='Test Set', height=width)\n",
    "    for i, v in enumerate(training_scores):\n",
    "        plt.text(v,ind[i]-width/2.5,'%.3f'%v)\n",
    "        plt.text(test_scores[i],ind[i]+width/1.5,'%.3f'%test_scores[i])\n",
    "        \n",
    "    plt.yticks(ind, regr_names) \n",
    "    plt.xlabel('MSE')\n",
    "    plt.title('Mean Squared Error Of All Regressors')\n",
    "    plt.legend(handles=[p1,p2])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().invert_xaxis()\n",
    "#     ax.yaxis.tick_right()\n",
    "    return mse_scores\n",
    "\n",
    "def plot_r2_score(X_train, y_train, X_test, y_test, all_regrs, regr_names, ax):\n",
    "    r2_scores = dict()\n",
    "    training_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for regr, regr_name in zip(all_regrs, regr_names):\n",
    "        train_preds = regr.predict(X_train)\n",
    "        test_preds = regr.predict(X_test)\n",
    "        train_score = sklearn.metrics.r2_score(y_train, train_preds)\n",
    "        test_score = sklearn.metrics.r2_score(y_test, test_preds)\n",
    "        training_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        r2_scores[regr_name] = test_score\n",
    "        \n",
    "    N = len(all_regrs)\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "#     p1 = plt.bar(ind, training_scores, width)\n",
    "#     p2 = plt.bar(ind+width, test_scores, width)\n",
    "#     plt.ylabel('Scores')\n",
    "#     plt.title('Scores by group and gender')\n",
    "#     plt.xticks(ind, regr_names,rotation='vertical')\n",
    "#     plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#     plt.legend((p1[0], p2[0]), ('Training', 'Test'))\n",
    "\n",
    "    p1 = plt.barh(ind-width/2, training_scores, align='center', label='Training Set', height=width)\n",
    "    p2 = plt.barh(ind+width/2, test_scores, align='center', label='Test Set', height=width)\n",
    "    for i, v in enumerate(training_scores):\n",
    "        plt.text(v+0.01,ind[i]-width/2.5,'%.3f'%v)\n",
    "        plt.text(max(test_scores[i],0)+0.01,ind[i]+width/1.5,'%.3f'%test_scores[i])\n",
    "\n",
    "    plt.yticks(ind, regr_names)\n",
    "    plt.xlabel('R² Score')\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.title('R² Scores Of All regressors')\n",
    "    plt.legend(handles=[p1,p2])\n",
    "    plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_xaxis()\n",
    "    ax.yaxis.tick_right()\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import from Utils folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# scipy.stats.randint(1,20)\n",
    "# scipy.stats.reciprocal(1.0, 100.),\n",
    "# scipy.stats.uniform(0.75, 1.25),\n",
    "def train_SVR(X_train, y_train):\n",
    "    print('Training SVR ...')\n",
    "    svr = SVR()\n",
    "    param_distributions = {\n",
    "        'kernel' : [DotProduct(),WhiteKernel(),RBF(),Matern(),RationalQuadratic()],\n",
    "        'C' : scipy.stats.reciprocal(1.0, 10.),\n",
    "#         'epsilon' : scipy.stats.uniform(0.1, 0.5),\n",
    "#         'gamma' : scipy.stats.reciprocal(0.01, 0.1),\n",
    "    }\n",
    "    randcv = RandomizedSearchCV(svr,param_distributions,n_iter=20,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_DecisionTree(X_train, y_train):\n",
    "    print('Training DecisionTree ...')\n",
    "    tree = DecisionTreeRegressor(random_state=0)\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(10,100)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(tree,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_RandomForest(X_train, y_train):\n",
    "    print('Training RandomForest ...')\n",
    "    forest = RandomForestRegressor(random_state=0, warm_start=True)\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(1,50),\n",
    "        'n_estimators' : scipy.stats.randint(100,200)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(forest,param_distributions,n_iter=10,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_AdaBoost(X_train, y_train):\n",
    "    print('Training AdaBoost ...')\n",
    "    boost = AdaBoostRegressor(random_state=0)\n",
    "    param_distributions = {\n",
    "        'loss' : ['linear', 'square', 'exponential'],\n",
    "        'learning_rate' : scipy.stats.uniform(0.75, 1.25),\n",
    "        'n_estimators' : scipy.stats.randint(40,100)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(boost,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_GaussianProcess(X_train, y_train):\n",
    "    print('Training GaussianProcess ...')\n",
    "    alpha = 1e-9\n",
    "    while(True):\n",
    "        try:\n",
    "            gaussian = GaussianProcessRegressor(normalize_y=True, random_state=0, optimizer=None, alpha=alpha)\n",
    "            param_distributions = {\n",
    "                'kernel' : [DotProduct(),WhiteKernel(),RBF(),Matern(),RationalQuadratic()],\n",
    "                'n_restarts_optimizer' : scipy.stats.randint(0,10),\n",
    "        #         'alpha' : scipy.stats.uniform(1e-9, 1e-8)\n",
    "            }\n",
    "            randcv = sklearn.model_selection.RandomizedSearchCV(gaussian,param_distributions,n_iter=5,cv=3,n_jobs=-1,random_state=0)\n",
    "            randcv.fit(X_train, y_train)\n",
    "            return randcv\n",
    "        except:\n",
    "            alpha *= 10\n",
    "\n",
    "def train_LinearRegression(X_train,y_train):\n",
    "    print('Training LinearRegression ...')\n",
    "    linear = LinearRegression(n_jobs=-1)\n",
    "    param_distributions = {\n",
    "        'normalize' : [True,False]\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(linear,param_distributions,n_iter=2,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv\n",
    "\n",
    "def train_NeuralNetwork(X_train, y_train):\n",
    "    print('Training NeuralNetwork ...')\n",
    "    nn = MLPRegressor(random_state=0, warm_start=True)\n",
    "    param_distributions = {\n",
    "        'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver' : ['lbfgs', 'adam'],\n",
    "        'hidden_layer_sizes' : [(100,50,25),(200,100,50)],\n",
    "        'learning_rate_init' : scipy.stats.uniform(0.001, 0.005),\n",
    "        'max_iter' : scipy.stats.randint(200,500)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(nn,param_distributions,n_iter=10,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_regrs(X_train, y_train, X_test, y_test):\n",
    "    all_regrs = []\n",
    "    regr_names = []\n",
    "\n",
    "    regr1 = train_SVR(X_train, y_train)\n",
    "    all_regrs.append(regr1.best_estimator_)\n",
    "    regr_names.append('SVR')\n",
    "\n",
    "    regr2 = train_DecisionTree(X_train, y_train)\n",
    "    all_regrs.append(regr2.best_estimator_)\n",
    "    regr_names.append('Decision Tree')\n",
    "\n",
    "    regr3 = train_RandomForest(X_train, y_train)\n",
    "    all_regrs.append(regr3.best_estimator_)\n",
    "    regr_names.append('Random Forest')\n",
    "\n",
    "    regr4 = train_AdaBoost(X_train, y_train)\n",
    "    all_regrs.append(regr4.best_estimator_)\n",
    "    regr_names.append('AdaBoost')\n",
    "\n",
    "    regr5 = train_GaussianProcess(X_train, y_train)\n",
    "    all_regrs.append(regr5.best_estimator_)\n",
    "    regr_names.append('Gaussian Process')\n",
    "\n",
    "    regr6 = train_LinearRegression(X_train, y_train)\n",
    "    all_regrs.append(regr6.best_estimator_)\n",
    "    regr_names.append('Linear Regression')\n",
    "\n",
    "    regr7 = train_NeuralNetwork(X_train, y_train)\n",
    "    all_regrs.append(regr7.best_estimator_)\n",
    "    regr_names.append('NeuralNetwork')\n",
    "\n",
    "    return all_regrs, regr_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sets\n",
    "DATA_PATH = 'dataset/'\n",
    "IMAGE_PATH = 'image/'\n",
    "files = []\n",
    "dfs = []\n",
    "dfs_test = []\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "file1 = 'winequality-red.csv'\n",
    "df1 = pd.read_csv(DATA_PATH+file1, delimiter=';',header=0)\n",
    "dfs.append(df1)\n",
    "dfs_test.append(None)\n",
    "files.append(file1)\n",
    "\n",
    "file1_2 = 'winequality-white.csv'\n",
    "df1_2 = pd.read_csv(DATA_PATH+file1_2, delimiter=';',header=0)\n",
    "dfs.append(df1_2)\n",
    "dfs_test.append(None)\n",
    "files.append(file1_2)\n",
    "\n",
    "#122 features, high accuracy\n",
    "file2 = 'communities.data'\n",
    "df2 = pd.read_csv(DATA_PATH+file2, delimiter=',', header=None)\n",
    "dfs.append(df2)\n",
    "dfs_test.append(None)\n",
    "files.append(file2)\n",
    "\n",
    "file3 = 'qsar_aquatic_toxicity.csv'\n",
    "df3 = pd.read_csv(DATA_PATH+file3, delimiter=';', header=None)\n",
    "dfs.append(df3)\n",
    "dfs_test.append(None)\n",
    "files.append(file3)\n",
    "\n",
    "#train+test, train 29 cols, test 28 cols\n",
    "file4 = 'Parkinson Speech_train_data.txt'\n",
    "# file4_test = 'Parkinson Speech_test_data.txt'\n",
    "df4 = pd.read_csv(DATA_PATH+file4, delimiter=',', header=None)\n",
    "df4 = df4.drop([28],axis=1)\n",
    "# df4_test = pd.read_csv(DATA_PATH+file4_test, delimiter=',', header=None)\n",
    "# df4_test = df4_test.drop(0,axis=1)\n",
    "dfs.append(df4)\n",
    "dfs_test.append(None)\n",
    "files.append(file4)\n",
    "\n",
    "#7 features, 12 evaluations, svr error\n",
    "file5 = 'Facebook_dataset.csv'\n",
    "df5 = pd.read_csv(DATA_PATH+file5, delimiter=';', header=0)\n",
    "df5 = pd.concat([df5.iloc[:,:7],df5.iloc[:,-5]],axis=1)\n",
    "dfs.append(df5)\n",
    "dfs_test.append(None)\n",
    "files.append(file5)\n",
    "\n",
    "#feature selection , high r2 score\n",
    "file6 = 'Bike Sharing_hour.csv'\n",
    "df6 = pd.read_csv(DATA_PATH+file6, delimiter=',', header=0, index_col=0)\n",
    "df6 = df6.drop(['dteday','casual','registered','yr','mnth'],axis=1)\n",
    "df6 = df6.sample(5000,random_state=0)\n",
    "dfs.append(df6)\n",
    "dfs_test.append(None)\n",
    "files.append(file6)\n",
    "\n",
    "# g1 g2 g3 corelated\n",
    "file7 = 'student-por.csv'\n",
    "df7 = pd.read_csv(DATA_PATH+file7, delimiter=';', header=0)\n",
    "dfs.append(df7)\n",
    "dfs_test.append(None)\n",
    "files.append(file7)\n",
    "\n",
    "file8 = 'Concrete_Data.xls'\n",
    "df8 = pd.read_excel(DATA_PATH+file8, header=0)\n",
    "dfs.append(df8)\n",
    "dfs_test.append(None)\n",
    "files.append(file8)\n",
    "\n",
    "#241600 rows, last four y , high score\n",
    "file9 = 'sgemm_product.csv'\n",
    "df9 = pd.read_csv(DATA_PATH+file9, delimiter=',', header=0)\n",
    "df9 = df9.sample(5000,random_state=0)\n",
    "df9 = df9.iloc[:,:-3]\n",
    "dfs.append(df9)\n",
    "dfs_test.append(None)\n",
    "files.append(file9)\n",
    "\n",
    "#dimension reduction to 15, good r2 score\n",
    "file10 = 'ACT2_competition_training.csv'\n",
    "df10 = pd.read_csv(DATA_PATH+file10, delimiter=',', header=0, index_col=0)\n",
    "df10['y'] = df10.Act\n",
    "df10 = df10.drop(['Act'],axis=1)\n",
    "dfs.append(df10)\n",
    "dfs_test.append(None)\n",
    "files.append(file10)\n",
    "\n",
    "file10_2 = 'ACT4_competition_training.csv'\n",
    "df10_2 = pd.read_csv(DATA_PATH+file10_2, delimiter=',', header=0, index_col=0)\n",
    "df10_2['y'] = df10_2.Act\n",
    "df10_2 = df10_2.drop(['Act'],axis=1)\n",
    "dfs.append(df10_2)\n",
    "dfs_test.append(None)\n",
    "files.append(file10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_mse = dict()\n",
    "all_r2 = dict()\n",
    "\n",
    "# i = 0\n",
    "# files=[files[i]]\n",
    "# dfs = [dfs[i]]\n",
    "# dfs_test = [dfs_test[i]]\n",
    "\n",
    "for df, df_test, file in zip(dfs,dfs_test,files):\n",
    "    print('Dataset: %s' % file)\n",
    "#     df.info()\n",
    "    df = replace_question_marks(df)\n",
    "    df_test = replace_question_marks(df_test)\n",
    "    \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "\n",
    "    if type(df_test) == type(None):\n",
    "        print('No given test set, dataset splitted')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "    else:\n",
    "        print('Loaded test set')\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        X_test = df_test.iloc[:,:-1]\n",
    "        y_test = df_test.iloc[:,-1]\n",
    "\n",
    "\n",
    "    X_train, X_test = encode_labels(X_train,X_test)\n",
    "    X_train, X_test = impute_value(X_train, X_test,'mean')\n",
    "    X_train, X_test = normalize_data(X_train, X_test)\n",
    "    X_train, X_test = dimension_reduction(X_train, X_test, n_components=15)\n",
    "    \n",
    "    all_regrs, regr_names = run_all_regrs(X_train, y_train, X_test, y_test)\n",
    "    mse, r2 = plot_all(X_train, y_train, X_test, y_test, all_regrs, regr_names, file)\n",
    "    \n",
    "    for k,v in mse.items():\n",
    "        if k not in all_mse:\n",
    "            all_mse[k] = list()\n",
    "        all_mse[k].append(v)\n",
    "    for k,v in r2.items():\n",
    "        if k not in all_r2:\n",
    "            all_r2[k] = list()\n",
    "        all_r2[k].append(v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in all_mse.items():\n",
    "    print('%s-mse:'%k,[\"%.3f\"%i for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in all_r2.items():\n",
    "    print('%s-r2:'%k,[\"%.3f\"%i for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  for k,v in all_r2.items():\n",
    "    print('%s avg r2: %.2f, avg mse: %.2f'%(k,np.mean(v),np.mean(all_mse[k])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
