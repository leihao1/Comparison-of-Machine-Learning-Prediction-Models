{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# import scikitplot as skplt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sets\n",
    "DATA_PATH = 'dataset/'\n",
    "IMAGE_PATH = 'image/'\n",
    "files = []\n",
    "dfs = []\n",
    "dfs_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(X):\n",
    "    try:\n",
    "        plt.imshow(np.array(X))\n",
    "    except:\n",
    "        plt.imshow(np.array(X).transpose(1,2,0))\n",
    "    plt.show()\n",
    "    \n",
    "def dimension_reduction(x_train, x_test, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x_train)\n",
    "    x_train= pca.transform(x_train)\n",
    "    x_test = pca.transform(x_test)\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR_10(training_batches=5, test_batches=1):\n",
    "    X_train=y_train=X_test=y_test = None\n",
    "    for b in range(1,training_batches+1):\n",
    "        with open(DATA_PATH+'data_batch_%s'%b,'rb') as file:\n",
    "            data = pickle.load(file,encoding='bytes')\n",
    "            X = np.array(data[b'data'])\n",
    "#             X = X.reshape(-1,3,32,32).transpose(0,2,3,1)\n",
    "            y = np.array(data[b'labels'])\n",
    "            try:\n",
    "                X_train = np.concatenate((X_train,X))\n",
    "                y_train = np.concatenate((y_train,y))\n",
    "            except:\n",
    "                print('first training batch')\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "                \n",
    "    for b in range(1,test_batches+1):\n",
    "        with open(DATA_PATH+'test_batch','rb') as file:\n",
    "            data = pickle.load(file,encoding='bytes')\n",
    "            X = np.array(data[b'data'])\n",
    "#             X = X.reshape(-1,3,32,32).transpose(0,2,3,1)\n",
    "            y = np.array(data[b'labels'])\n",
    "            try:\n",
    "                X_test = np.concatenate((X_test,X))\n",
    "                y_test = np.concatenate((y_test,y))\n",
    "            except:\n",
    "                print('first test batch')\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DecisionTree(X_train, y_train):\n",
    "    print('Training DecisionTree ...')\n",
    "    tree = DecisionTreeClassifier(random_state=0)\n",
    "    param_distributions = {\n",
    "#         'max_depth' : scipy.stats.randint(100,200)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(tree,param_distributions,n_iter=1,cv=3,n_jobs=-1,\n",
    "                                                        iid=False,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    print('Training finished')\n",
    "    return randcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_train,y_train,X_test,y_test = load_CIFAR_10(1,1)\n",
    "# X_train,X_test = dimension_reduction(X_train,X_test,100)\n",
    "# %%time\n",
    "# trees = train_DecisionTree(X_train,y_train)\n",
    "# clf = trees.best_estimator_\n",
    "# clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_images(X):\n",
    "    return X.reshape(-1,3,32,32).transpose(0,2,3,1)\n",
    "\n",
    "def transform_data(x_train,x_test):\n",
    "    # tranform functions\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#         Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#         CenterCrop(32),\n",
    "        ])\n",
    "    \n",
    "    x_train = convert_to_images(x_train)\n",
    "    x_test = convert_to_images(x_test)\n",
    "#     plot_image(x_train[0])\n",
    "    #transformed dateset\n",
    "    x_train = np.array([np.array(transform(x)) for x in x_train])\n",
    "    x_test = np.array([np.array(transform(x)) for x in x_test])\n",
    "#     plot_image(x_train[0])\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training batch\n",
      "first test batch\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test = load_CIFAR_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = transform_data(X_train,X_test)\n",
    "# X_train = X_train.reshape(-1,3,32,32)\n",
    "# X_test = X_test.reshape(-1,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.astype(np.float32))\n",
    "X_test = torch.tensor(X_test.astype(np.float32))\n",
    "y_train = torch.tensor(y_train.astype(np.int64))\n",
    "y_test = torch.tensor(y_test.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn parameters\n",
    "image_channels = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "num_filters = 32\n",
    "num_filters2 = 64\n",
    "num_filters3 = 128\n",
    "filter_size = 5\n",
    "pool_size = 2\n",
    "# final_input = (((image_width+1-filter_size)//pool_size+1-filter_size)//pool_size)**2*num_filters2#without padding\n",
    "final_input = (image_width//pool_size//pool_size//pool_size)**2*num_filters3#with padding\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(image_channels, num_filters, filter_size, padding=filter_size//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(pool_size, pool_size),\n",
    "    \n",
    "    torch.nn.Conv2d(num_filters, num_filters2, filter_size, padding=filter_size//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(pool_size, pool_size),\n",
    "    \n",
    "    torch.nn.Conv2d(num_filters2, num_filters3, filter_size, padding=filter_size//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(pool_size, pool_size),\n",
    "    \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(final_input, final_input//2),\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(final_input//2, final_input//4),\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(final_input//4, final_input//16),\n",
    "    torch.nn.ReLU(),\n",
    "\n",
    "    torch.nn.Linear(final_input//16,10),\n",
    ")\n",
    "\n",
    "# model2 = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(final_input, final_input//2),\n",
    "#     torch.nn.ReLU(),\n",
    "    \n",
    "#     torch.nn.Linear(final_input//2, final_input//4),\n",
    "#     torch.nn.ReLU(),\n",
    "    \n",
    "#     torch.nn.Linear(final_input//4, final_input//16),\n",
    "#     torch.nn.ReLU(),\n",
    "\n",
    "#     torch.nn.Linear(final_input//16,10),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_models(model,model2,X_train,X_test,y_train,y_test,final_input,batch_size,num_epoch):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-3)\n",
    "#     optimizer2 = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-3)\n",
    "\n",
    "    #mini-batch training loop\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            X, y = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             optimizer2.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "#             X2 = model(X).view(-1,final_input)\n",
    "#             y_pred = model2(X2)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             optimizer2.step()\n",
    "\n",
    "            #avg batch loss\n",
    "            running_loss += loss.item()\n",
    "            batch = 500\n",
    "            if (i+1) % batch == 0:\n",
    "                print('Epoch: %d, Batch: %5d had avg loss: %.3f'%(epoch+1,i+1,running_loss/batch))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "#     torch.save(model2.state_dict(), 'model2.pt')\n",
    "    print('Training Finished')\n",
    "#     return model,model2\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models not found, start straining...\n",
      "Epoch: 1, Batch:   500 had avg loss: 2.303\n",
      "Epoch: 1, Batch:  1000 had avg loss: 2.301\n",
      "Epoch: 1, Batch:  1500 had avg loss: 2.298\n",
      "Epoch: 1, Batch:  2000 had avg loss: 2.287\n",
      "Epoch: 1, Batch:  2500 had avg loss: 2.162\n",
      "Epoch: 1, Batch:  3000 had avg loss: 1.995\n",
      "Epoch: 2, Batch:   500 had avg loss: 1.828\n",
      "Epoch: 2, Batch:  1000 had avg loss: 1.738\n",
      "Epoch: 2, Batch:  1500 had avg loss: 1.689\n",
      "Epoch: 2, Batch:  2000 had avg loss: 1.657\n",
      "Epoch: 2, Batch:  2500 had avg loss: 1.578\n",
      "Epoch: 2, Batch:  3000 had avg loss: 1.555\n",
      "Training Finished\n",
      "CPU times: user 1h 22min 49s, sys: 1min 41s, total: 1h 24min 30s\n",
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size=16\n",
    "num_epoch=50\n",
    "# create torch Dataset class from tensors\n",
    "train_set = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_set = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "#     try:\n",
    "#         model2.load_state_dict(torch.load('model2.pt'))\n",
    "#     except:\n",
    "#         pass\n",
    "except:\n",
    "    print('Models not found, start straining...')\n",
    "#     model,model2 = train_models(model,model2,X_train,X_test,y_train,y_test,final_input,20,2)    \n",
    "    model = train_models(model,None,X_train,X_test,y_train,y_test,final_input,batch_size,num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 42.89 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        X, y = data\n",
    "#         y_proba = model2(model(X).view(-1,final_input))\n",
    "        y_proba = model(X)\n",
    "        _, y_pred = torch.max(y_proba.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 46 %\n",
      "Accuracy of   car : 53 %\n",
      "Accuracy of  bird : 14 %\n",
      "Accuracy of   cat : 19 %\n",
      "Accuracy of  deer : 48 %\n",
      "Accuracy of   dog : 64 %\n",
      "Accuracy of  frog : 50 %\n",
      "Accuracy of horse : 32 %\n",
      "Accuracy of  ship : 60 %\n",
      "Accuracy of truck : 39 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        X, y = data\n",
    "#         y_proba = model2(model(X).view(-1,final_input))\n",
    "        y_proba = model(X)\n",
    "        _, y_pred = torch.max(y_proba, 1)\n",
    "        c = (y_pred == y).squeeze()\n",
    "        for i in range(4):\n",
    "            label = y[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
