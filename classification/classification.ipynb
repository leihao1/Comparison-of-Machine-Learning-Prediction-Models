{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Utils')\n",
    "from pre_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import scikitplot as skplt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import datasets \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC and PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(X_test, y_test, all_clfs, clf_names, file_name):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.suptitle(\"Dataset: %s\"%file_name, size=16)\n",
    "    plt.subplot(121)\n",
    "    roc_scores = plot_roc_curve(X_test, y_test, all_clfs, clf_names)\n",
    "    plt.subplot(122)\n",
    "    pr_scores = plot_pr_curve(X_test, y_test, all_clfs, clf_names)\n",
    "    plt.savefig(image_path+file.split('.')[0]+'-roc_pr')\n",
    "    plt.show()\n",
    "    return roc_scores, pr_scores\n",
    "\n",
    "#plot tools\n",
    "def plot_roc_curve(X_test, y_test, all_clfs, clf_names):\n",
    "    roc_scores = dict()\n",
    "#     plt.figure(figsize=(10,8))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    np.random.seed(0)\n",
    "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    n_classes = y_test.shape[1]\n",
    "\n",
    "    for clf, clf_name in zip(all_clfs, clf_names):\n",
    "        #two classes \n",
    "        if n_classes <= 2:\n",
    "            probs = clf.predict_proba(X_test)\n",
    "            preds = probs[:,1]\n",
    "            fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, preds)\n",
    "            roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "            roc_scores[clf_name] = roc_auc\n",
    "            plt.plot(fpr, tpr, 'b', label = '%s (AUC = %0.3f)' % (clf_name,roc_auc), c=np.random.rand(3,))\n",
    "            plt.legend(loc = 'lower right')         \n",
    "        #multi classes\n",
    "        else:\n",
    "            y_score = clf.predict_proba(X_test)\n",
    "            # Compute ROC curve and ROC area for each class\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            for i in range(n_classes):\n",
    "                fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:,i],y_score[:,i])\n",
    "                roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "            # Compute micro-average ROC curve and ROC area\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = sklearn.metrics.roc_curve(y_test.ravel(),y_score.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "            roc_scores[clf_name] = roc_auc[\"micro\"]\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"], 'b', label = '%s (Micro AUC = %0.3f)' % \n",
    "                     (clf_name,roc_auc[\"micro\"]), c=np.random.rand(3,))\n",
    "            plt.legend(loc = 'lower right')        \n",
    "    return roc_scores\n",
    "\n",
    "def plot_pr_curve(X_test, y_test, all_clfs, clf_names):\n",
    "    pr_scores = dict()\n",
    "#     plt.figure(figsize=(10,8))\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.plot([0, 1], [1, 0],'r--')\n",
    "    plt.xlim([0, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    np.random.seed(0)\n",
    "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    n_classes = y_test.shape[1]\n",
    "    \n",
    "    for clf, clf_name in zip(all_clfs, clf_names):\n",
    "        #two classes\n",
    "        if n_classes <= 2:\n",
    "            probs = clf.predict_proba(X_test)\n",
    "            preds = probs[:,1]\n",
    "            precision, recall, _ = sklearn.metrics.precision_recall_curve(y_test, preds)\n",
    "            pr_auc = sklearn.metrics.auc(recall, precision)\n",
    "            pr_scores[clf_name] = pr_auc\n",
    "            plt.plot(recall, precision, 'b', label = '%s (AUC = %0.3f)' % (clf_name,pr_auc), c=np.random.rand(3,))\n",
    "            plt.legend(loc = 'lower left')\n",
    "        #multi classes\n",
    "        else:\n",
    "            y_score = clf.predict_proba(X_test)\n",
    "            # Compute ROC curve and ROC area for each class\n",
    "            precision = dict()\n",
    "            recall = dict()\n",
    "            pr_auc = dict()\n",
    "            for i in range(n_classes):\n",
    "                precision[i], recall[i], _ = sklearn.metrics.precision_recall_curve(y_test[:,i],y_score[:,i])\n",
    "                pr_auc[i] = sklearn.metrics.auc(recall[i], precision[i])\n",
    "            # Compute micro-average ROC curve and ROC area\n",
    "            precision[\"micro\"], recall[\"micro\"], _ = sklearn.metrics.precision_recall_curve(y_test.ravel(),y_score.ravel())\n",
    "            pr_auc[\"micro\"] = sklearn.metrics.auc(recall[\"micro\"], precision[\"micro\"])  \n",
    "            pr_scores[clf_name] = pr_auc[\"micro\"]\n",
    "            plt.plot(recall[\"micro\"], precision[\"micro\"], 'b', label = '%s (Micro AUC = %0.3f)' % \n",
    "                     (clf_name,pr_auc[\"micro\"]), c=np.random.rand(3,))\n",
    "            plt.legend(loc = 'lower left')\n",
    "    return pr_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-porcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import from Utils folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "def train_KNN(X_train, y_train):\n",
    "    print('Training KNN ...')\n",
    "    knn = KNeighborsClassifier()\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'n_neighbors' : scipy.stats.randint(1,20)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        knn,param_distributions,n_iter=20,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_\n",
    "\n",
    "def train_SVM(X_train, y_train):\n",
    "    print('Training SVM ...')\n",
    "    svm = SVC(kernel='rbf', probability=True, cache_size=3000, random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'C' : scipy.stats.reciprocal(1.0, 100.),\n",
    "        'gamma' : scipy.stats.reciprocal(0.01, 10.),\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        svm,param_distributions,n_iter=20,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_\n",
    "\n",
    "def train_DecisionTree(X_train, y_train):\n",
    "    print('Training DecisionTree ...')\n",
    "    tree = DecisionTreeClassifier(random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(10,1000)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        tree,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_\n",
    "\n",
    "def train_RandomForest(X_train, y_train):\n",
    "    print('Training RandomForest ...')\n",
    "    forest = RandomForestClassifier(random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'max_depth' : scipy.stats.randint(10,100),\n",
    "        'n_estimators' : scipy.stats.randint(100,1000)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        forest,param_distributions,n_iter=10,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_\n",
    "\n",
    "def train_AdaBoost(X_train, y_train):\n",
    "    print('Training AdaBoost ...')\n",
    "    boost = AdaBoostClassifier(random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'learning_rate' : scipy.stats.uniform(0.75, 1.25),\n",
    "        'n_estimators' : scipy.stats.randint(40,70)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        boost,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_    \n",
    "\n",
    "def train_LogisticRegression(X_train, y_train):\n",
    "    print('Training LogisticRegression ...')\n",
    "    lr = LogisticRegression(solver='liblinear', multi_class='auto', random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'C' : scipy.stats.reciprocal(1.0, 1000.),\n",
    "        'max_iter' : scipy.stats.randint(100,1000)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        lr,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_ \n",
    "\n",
    "def train_GaussianNaiveBayes(X_train, y_train):\n",
    "    print('Training GaussianNaiveBayes ...')\n",
    "    gaussian = GaussianNB()\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'var_smoothing' : scipy.stats.uniform(1e-10, 1e-9),\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        gaussian,param_distributions,n_iter=30,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_ \n",
    "\n",
    "def train_NeuralNetwork(X_train, y_train):\n",
    "    print('Training NeuralNetwork ...')\n",
    "    nn = MLPClassifier(solver='adam', random_state=0)\n",
    "#     scoring = ['roc_auc']\n",
    "    param_distributions = {\n",
    "        'hidden_layer_sizes' : [(100,50,10)],\n",
    "        'learning_rate_init' : scipy.stats.uniform(0.001, 0.005),\n",
    "        'max_iter' : scipy.stats.randint(200,500)\n",
    "    }\n",
    "    randcv = sklearn.model_selection.RandomizedSearchCV(\n",
    "        nn,param_distributions,n_iter=10,cv=3,n_jobs=-1,random_state=0)\n",
    "    randcv.fit(X_train, y_train)\n",
    "    return randcv.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_all_clfs(X_train, y_train, X_test, y_test):\n",
    "    all_clfs = []\n",
    "    clf_names = []\n",
    "\n",
    "    clf1 = train_KNN(X_train, y_train)\n",
    "    all_clfs.append(clf1)\n",
    "    clf_names.append('KNN')\n",
    "\n",
    "    clf2 = train_SVM(X_train, y_train)\n",
    "    all_clfs.append(clf2)\n",
    "    clf_names.append('SVM')\n",
    "\n",
    "    clf3 = train_DecisionTree(X_train, y_train)\n",
    "    all_clfs.append(clf3)\n",
    "    clf_names.append('Decision Tree')\n",
    "\n",
    "    clf4 = train_RandomForest(X_train, y_train)\n",
    "    all_clfs.append(clf4)\n",
    "    clf_names.append('Random Forest')\n",
    "\n",
    "    clf5 = train_AdaBoost(X_train, y_train)\n",
    "    all_clfs.append(clf5)\n",
    "    clf_names.append('AdaBoost')\n",
    "\n",
    "    clf6 = train_LogisticRegression(X_train, y_train)\n",
    "    all_clfs.append(clf6)\n",
    "    clf_names.append('Logistic regression')\n",
    "\n",
    "    clf7 = train_GaussianNaiveBayes(X_train, y_train)\n",
    "    all_clfs.append(clf7)\n",
    "    clf_names.append('Gaussian Naive Bayes')\n",
    "\n",
    "    clf8 = train_NeuralNetwork(X_train, y_train)\n",
    "    all_clfs.append(clf8)\n",
    "    clf_names.append('NeuralNetwork')\n",
    "\n",
    "    return all_clfs, clf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(file, df, df_test):\n",
    "    print(\"Dataset : %s\" % file)\n",
    "    #     df.info()\n",
    "    check_class_distribution(df)\n",
    "    df = replace_question_marks(df)\n",
    "    df_test = replace_question_marks(df_test)\n",
    "    \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    assert len(y.shape) == 1, \"wrong shape of y\"\n",
    "\n",
    "    if type(df_test) == type(None):\n",
    "        print('No default test set, dataset splitted')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "    else:\n",
    "        print('Loaded given test set')\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        X_test = df_test.iloc[:,:-1]\n",
    "        y_test = df_test.iloc[:,-1]      \n",
    "\n",
    "    X_train, X_test = encode_labels(X_train,X_test)\n",
    "    X_train, X_test = dimension_reduction(X_train,X_test)\n",
    "\n",
    "    #     X_train, X_test = impute_value(X_train, X_test, strategy = 'most_frequent')\n",
    "    X_train, X_test = impute_value(X_train, X_test,strategy = 'mean')\n",
    "    X_train, X_test = normalize_data(X_train, X_test)\n",
    "\n",
    "    y_train, y_test = encode_labels(y_train, y_test, -1)\n",
    "\n",
    "    all_clfs, clf_names = run_all_clfs(X_train, y_train, X_test, y_test)\n",
    "    roc_scores, pr_scores = plot_all(X_test, y_test, all_clfs, clf_names, file)\n",
    "    return roc_scores, pr_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/'\n",
    "image_path = 'image/'\n",
    "\n",
    "file1 = 'messidor_features.arff'\n",
    "data1 = arff.loadarff(data_path+file1)\n",
    "df1 = pd.DataFrame(data1[0])\n",
    "\n",
    "file2 = 'default of credit card clients.xls'\n",
    "df2 = pd.read_excel(data_path+file2,index_col=0,header=1)\n",
    "df2 = df2.sample(5000,random_state=0)\n",
    "\n",
    "file3 = 'breast-cancer-wisconsin.data'\n",
    "df3 = pd.read_csv(data_path+file3,header=None, index_col=0)\n",
    "df3 = df3[df3[6]!='?'].astype('int64')\n",
    "\n",
    "file3_2 = 'wdbc.data'\n",
    "df3_2 = pd.read_csv(data_path+file3_2,header=None, index_col=0)\n",
    "df3_2 = reorder_columns(df3_2)\n",
    "\n",
    "file3_3 = 'wpbc.data'\n",
    "df3_3 = pd.read_csv(data_path+file3_3,header=None, index_col=0)\n",
    "df3_3 = reorder_columns(df3_3)\n",
    "df3_3 = df3_3[df3_3[34]!='?']\n",
    "df3_3[34] = df3_3[34].astype('int64')\n",
    "\n",
    "file4 = 'australian.dat'\n",
    "df4 = pd.read_csv(data_path+file4,header=None,sep=' ')\n",
    "df4 = reorder_columns(df4)\n",
    "\n",
    "file5 = 'german.data-numeric'\n",
    "df5 = pd.read_csv(data_path+file5,header=None,sep=' +', engine='python')\n",
    "\n",
    "#multi-class\n",
    "file6 = 'Faults.NNA'\n",
    "df6 = pd.read_csv(data_path+file6,header=None,sep='\\t', engine='python')\n",
    "cats  = df6.iloc[:,-7:].idxmax(axis=1)\n",
    "df6 = df6.drop(df6.columns[-7:], axis=1)\n",
    "df6['class'] = cats\n",
    "\n",
    "file7 = 'adult.data'\n",
    "file7_test = 'adult.test'\n",
    "df7 = pd.read_csv(data_path+file7,header=None,sep=', ', engine='python')\n",
    "df7 = df7.sample(5000, random_state=0)\n",
    "df7_test = pd.read_csv(data_path+file7_test,header=None,sep=', ', engine='python',skiprows=1)\n",
    "\n",
    "#multi-class\n",
    "file8 = 'yeast.data'\n",
    "df8 = pd.read_csv(data_path+file8,header=None,sep=' +', index_col=0, engine='python')\n",
    "\n",
    "#error dataset\n",
    "file9 = 'ThoraricSurgery.arff'\n",
    "data9 = arff.loadarff(data_path+file9)\n",
    "df9 = pd.DataFrame(data9[0])\n",
    "\n",
    "file10 = 'seismic-bumps.arff'\n",
    "data10, meta = arff.loadarff(data_path+file10)\n",
    "df10 = pd.DataFrame(data10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "files = [file1,file2,file3,file3_2,file3_3,file4,file5,file6,file7,file8,file9,file10]\n",
    "dfs = [df1,df2,df3,df3_2,df3_3,df4,df5,df6,df7,df8,df9,df10]\n",
    "dfs_test = [None,None,None,None,None,None,None,None,df7_test,None,None,None]\n",
    "\n",
    "clfs_all_roc_scores = dict()\n",
    "clfs_all_pr_scores = dict()\n",
    "\n",
    "# i = 0\n",
    "# files=[files[i]]\n",
    "# dfs = [dfs[i]]\n",
    "# dfs_test = [dfs_test[i]]\n",
    "\n",
    "for file,df,df_test in zip(files,dfs,dfs_test):\n",
    "    roc_scores, pr_scores = make_classification(file, df, df_test)\n",
    "    for k,v in roc_scores.items():\n",
    "        if k not in clfs_all_roc_scores:\n",
    "            clfs_all_roc_scores[k] = list()\n",
    "        clfs_all_roc_scores[k].append(v)\n",
    "    for k,v in pr_scores.items():\n",
    "        if k not in clfs_all_pr_scores:\n",
    "            clfs_all_pr_scores[k] = list()\n",
    "        clfs_all_pr_scores[k].append(v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in clfs_all_roc_scores.items():\n",
    "    print('%s-roc:'%k,[\"%.3f\"%i for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in clfs_all_pr_scores.items():\n",
    "    print('%s-pr:'%k,[\"%.3f\"%i for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in clfs_all_roc_scores.items():\n",
    "    print('%s avg roc_auc: %.2f, avg pr_auc: %.2f'%(k,np.mean(v),np.mean(clfs_all_pr_scores[k])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
